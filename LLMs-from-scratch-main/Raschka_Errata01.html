UTF-8
<html>
<head>
  <link href="https://fonts.googleapis.com/css?family=Lato:300,400,700,900,300italic,400italic,700italic,900italic&amp;subset=latin" rel="stylesheet" type="text/css">
  <style>
    body {
      color: #333;
      font-size: 10px;
      padding: 40px;
      text-align: center;
      font-family: Lato, sans-serif;
      font-weight: 300;
      line-height: 1.5em;
    }
    .manning-logo {
      width: 75%;
      max-width: 500px;
      margin: auto;
    }
    .errata-body {
      text-align: left;
      width: 75%;
      max-width: 960px;
      margin: 40px auto;
      padding-top: calc(40px - 0.5em);
      padding-bottom: calc(20px - 0.5em);
      border-top: solid thin #333;
      border-bottom: solid thin #333;
      font-size: 2em;
      line-height: 1.5em;
    }
    .errata-body > * {
      padding: 0;
      margin: 0;
      margin-bottom: 20px;
    }
    .copyright {
      font-size: 1.25em;
      padding: 40px auto;
    }
      code {
          font-family: courier;
          color: grey
          
      }
  </style>
</head>
<body>
  <div class="manning-logo">
    <svg xmlns:svg="http://www.w3.org/2000/svg" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 350 45" version="1.1">
      <style>
          .style0{
          fill:	#333;
          fill:	#333;
          fill-rule:	evenodd;
          }
      </style><defs/>
      <g transform="translate(0,-1007.3622)">
          <g transform="matrix(1.0016907,0,0,0.99930881,500.03637,1045.9036)">
              <g transform="matrix(1.25,0,0,-1.25,-406.19425,6.278225)">
                  <path d="m 0 0 -2.975 0 0 11.882 -1.961 -5.856 -1.324 0 -1.963 5.856 0 -11.882 -2.941 0 0 22.267 1.471 0 L -5.6 10.67 -1.473 22.267 0 22.267 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-395.52288,-1.7259)">
                  <path d="M 0 0 -1.213 9.315 -2.424 0 0 0 z m 3.721 -6.403 -2.856 0 -0.549 3.95 -3.056 0 -0.52 -3.95 -2.713 0 3.375 22.266 2.946 0 3.373 -22.266 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-375.8695,6.278225)">
                  <path d="m 0 0 -1.473 0 -4.99 12.689 0 -12.689 -2.943 0 0 22.267 1.472 0 4.961 -12.778 0 12.778 2.973 0 L 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-359.28263,6.278225)">
                  <path d="m 0 0 -1.471 0 -4.99 12.689 0 -12.689 -2.945 0 0 22.267 1.472 0 4.961 -12.778 0 12.778 2.973 0 L 0 0 z" class="style0"/>
              </g>
              <path d="m -350.628 6.278 -3.826 0 0 -27.834 3.826 0 0 27.834 z" class="style0"/>
              <g transform="matrix(1.25,0,0,-1.25,-334.0775,6.278225)">
                  <path d="m 0 0 -1.471 0 -4.992 12.689 0 -12.689 -2.943 0 0 22.267 1.472 0 4.961 -12.778 0 12.778 2.973 0 L 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-317.63463,5.45185)">
                  <path d="m 0 0 c -0.867 -0.538 -1.961 -0.809 -3.289 -0.809 -4.311 0 -6.463 3.772 -6.463 11.309 0 7.501 2.039 11.253 6.115 11.253 0.674 0 1.321 -0.087 1.934 -0.262 l 0 -2.364 c -0.539 0.113 -0.992 0.17 -1.356 0.17 -2.406 0 -3.605 -2.931 -3.605 -8.797 0 -5.902 1.164 -8.856 3.49 -8.856 0.137 0 0.24 0.01 0.316 0.03 l 0 4.615 c 0 1.092 -0.125 1.846 -0.375 2.252 -0.191 0.325 -0.519 0.488 -0.978 0.488 -0.232 0 -0.397 -0.01 -0.492 -0.028 l 0 2.423 c 0.326 0.056 0.662 0.086 1.01 0.086 C -1.23 11.51 0 9.684 0 6.028 L 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-299.75388,-13.117525)">
                  <path d="m 0 0 c 0 2.846 -0.854 4.271 -2.563 4.271 l 0 -9.81 c 0.479 0.077 0.977 0.508 1.497 1.301 C -0.355 -3.164 0 -1.751 0 0 m 3.087 -0.029 c 0 -2.556 -0.788 -4.655 -2.364 -6.285 -1.039 -1.062 -2.133 -1.746 -3.286 -2.051 l 0 -7.152 -3.06 0 0 22.267 2.335 0 c 1.829 0 3.3 -0.487 4.417 -1.469 1.305 -1.175 1.958 -2.944 1.958 -5.31" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-281.10875,-0.567525)">
                  <path d="m 0 0 c 0 -1.844 -0.471 -3.277 -1.412 -4.298 -0.83 -0.883 -1.875 -1.326 -3.145 -1.326 -1.252 0 -2.291 0.443 -3.115 1.326 -0.924 1.021 -1.387 2.454 -1.387 4.298 l 0 16.79 3.059 0 0 -17.392 c 0 -0.83 0.145 -1.463 0.432 -1.907 0.289 -0.439 0.625 -0.663 1.011 -0.663 0.385 0 0.731 0.224 1.037 0.663 0.309 0.444 0.463 1.077 0.463 1.907 l 0 17.392 L 0 16.79 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-268.9555,-2.9149)">
                  <path d="M 0 0 C 0 2.057 -0.549 3.633 -1.645 4.73 -2.051 4.596 -2.465 4.527 -2.887 4.527 l 0 -9.432 c 0.791 0 1.434 0.337 1.934 1.009 C -0.32 -3.05 0 -1.751 0 0 m -0.607 9.749 c 0 1.751 -0.758 2.654 -2.28 2.711 l 0 -5.394 c 1.522 0.039 2.28 0.933 2.28 2.683 M 3.115 0 c 0 -2.674 -0.711 -4.638 -2.135 -5.886 -1.134 -0.981 -2.644 -1.468 -4.527 -1.468 l -2.396 0 0 22.266 2.451 0 c 1.597 0 2.931 -0.394 4.01 -1.182 C 1.787 12.808 2.422 11.48 2.422 9.749 2.422 8.02 1.865 6.719 0.748 5.855 1.307 5.412 1.807 4.771 2.25 3.924 2.824 2.75 3.115 1.441 3.115 0" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-251.973,6.278225)">
                  <path d="m 0 0 -7.182 0 0 22.267 3.057 0 0 -19.818 L 0 2.449 0 0 z" class="style0"/>
              </g>
              <path d="m -244.834 6.278 -3.824 0 0 -27.834 3.824 0 0 27.834 z" class="style0"/>
              <g transform="matrix(1.25,0,0,-1.25,-231.13075,6.20985)">
                  <path d="m 0 0 c -0.463 -0.135 -0.963 -0.202 -1.502 -0.202 -4.078 0 -6.115 3.771 -6.115 11.308 0 7.501 2.037 11.253 6.115 11.253 0.539 0 1.039 -0.07 1.502 -0.204 L 0 19.76 c -0.52 0.096 -0.867 0.143 -1.039 0.143 -2.326 0 -3.488 -2.93 -3.488 -8.797 0 -5.902 1.181 -8.856 3.547 -8.856 0.228 0 0.556 0.041 0.98 0.115 L 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-221.61175,-1.7259)">
                  <path d="M 0 0 -1.211 9.315 -2.424 0 0 0 z m 3.721 -6.403 -2.856 0 -0.545 3.95 -3.06 0 -0.52 -3.95 -2.711 0 3.375 22.266 2.942 0 3.375 -22.266 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-203.97988,-18.4899)">
                  <path d="m 0 0 -2.973 0 0 -19.814 -3.056 0 0 19.814 -2.971 0 0 2.452 9 0 L 0 0 z" class="style0"/>
              </g>
              <path d="m -196.839 6.278 -3.824 0 0 -27.834 3.824 0 0 27.834 z" class="style0"/>
              <g transform="matrix(1.25,0,0,-1.25,-183.82125,-7.637775)">
                  <path d="m 0 0 c 0 5.884 -0.664 8.825 -1.992 8.825 -1.326 0 -1.99 -2.941 -1.99 -8.825 0 -5.884 0.664 -8.828 1.99 -8.828 C -0.664 -8.828 0 -5.884 0 0 m 3.057 0 c 0 -4.367 -0.537 -7.453 -1.616 -9.26 -0.789 -1.344 -1.933 -2.02 -3.433 -2.02 -1.5 0 -2.654 0.676 -3.461 2.02 -1.076 1.807 -1.615 4.893 -1.615 9.26 0 4.365 0.539 7.454 1.615 9.259 0.807 1.345 1.961 2.022 3.461 2.022 1.5 0 2.644 -0.677 3.433 -2.022 C 2.52 7.454 3.057 4.365 3.057 0" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-164.02625,6.278225)">
                  <path d="m 0 0 -1.471 0 -4.992 12.689 0 -12.689 -2.939 0 0 22.267 1.468 0 4.961 -12.778 0 12.778 2.973 0 L 0 0 z" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-149.78313,-0.71765)">
                  <path d="m 0 0 c 0 -1.887 -0.564 -3.35 -1.701 -4.387 -1.002 -0.905 -2.221 -1.357 -3.666 -1.357 -1.192 0 -2.192 0.26 -2.998 0.779 l 1.068 2.164 c 0.518 -0.306 1.164 -0.461 1.93 -0.461 0.597 0 1.117 0.231 1.56 0.694 0.498 0.535 0.75 1.297 0.75 2.278 0 1.593 -0.826 3.566 -2.482 5.912 -0.981 1.402 -1.672 2.571 -2.076 3.505 -0.403 0.934 -0.608 1.929 -0.608 2.988 0 1.441 0.43 2.584 1.285 3.43 0.856 0.848 2.004 1.272 3.448 1.272 1.002 0 1.799 -0.194 2.392 -0.579 l -1.037 -2.134 c -0.443 0.19 -0.906 0.289 -1.385 0.289 -0.5 0 -0.908 -0.18 -1.226 -0.536 -0.317 -0.353 -0.475 -0.83 -0.475 -1.426 0 -0.734 0.221 -1.519 0.662 -2.365 C -4.27 9.47 -3.711 8.585 -2.885 7.412 -1.883 5.97 -1.172 4.758 -0.75 3.775 -0.25 2.564 0 1.303 0 0" class="style0"/>
              </g>
              <g transform="matrix(1.25,0,0,-1.25,-490.70113,-38.568025)">
                  <path d="m 0 0 20.963 0 0 -22.049 C 22.164 -14.783 23.195 -7.35 24.311 0 l 20.964 0 0 -36.024 -19.783 0 0 30.906 C 23.877 -15.348 22.32 -25.64 20.865 -36.024 l -19.681 0 0 32.187 c -2.13 -10.642 -4.026 -21.507 -6.108 -32.187 l -1.869 0 c 2.242 11.898 4.472 23.809 6.693 35.728 C -0.109 -0.153 -0.115 -0.017 0 0" class="style1"/>
              </g>
          </g>
      </g>
    </svg>
  </div>
  <div class="errata-body">
       <img  style="float:left;width:150px;height:180px;border:1px solid black;margin:0px 10px 0px 0px;" src="https://images.manning.com/book/0/554d3c2-99e0-4445-8966-3fc874fe7d75/Raschka-HI.png">
        
   
    Errata: March 12, 2025<br><br>
      
  Thank you for purchasing <a href="https://www.manning.com/books/build-a-large-language-model-from-scratch" target="_blank"><i>Build a Large Language Model (From Scratch)</i></a>. Please post errata not listed below in this book's <a href="https://livebook.manning.com/forum?product=raschka&page=1" target="_blank">LiveBook Errata thread</a>. We'll update this list as necessary. Corrections are made to all formats during the book's second printing. Thank you!
        <br><br><br><br>
     
	<p><strong>Front Matter, Section <q>preface</q>, page xi</p></strong>
	<p>Paragraph 3: <q>As their name implies, a hallmark of LLMs is that they are...</q> should be <q>As their name implies, LLMs are...</q></p>

	<p><strong>Front Matter, Section <q>Who should read this book</q>, page xvi</p></strong>
	<p>Paragraph 5: <q>However, advanced mathematical...</q> should be <q>Advanced mathematical...</q></p>

	<p><strong>Chapter 1, page 2</p></strong>
	<p>Paragraph 5: <q>The success behind LLMs...</q> should be <q>The success of LLMs...</q></p>

	<p><strong>Chapter 1, Section <q>1.1 What is an LLM?</q>, page 2</p></strong>
	<p>Paragraph 2: <q>Yet, it is a very simple task...</q> should be <q>It is a very simple task...</q></p>

	<p><strong>Chapter 1, Section <q>1.1 What is an LLM?</q>, page 3</p></strong>
	<p>Paragraph 4: <q>Since LLMs are capable of generating text, LLMs are...</q> should be <q>Since LLMs are capable of generating text, they are...</q></p>

	<p><strong>Chapter 1, Section <q>1.2 Applications of LLMs</q>, page 4</p></strong>
	<p>Paragraph 4: <q>...that can generate texts.</q> should be <q>...that can generate text.</q></p>

	<p><strong>Chapter 1, Section <q>1.3 Stages of building and using LLMs</q>, page 5</p></strong>
	<p>Paragraph 1: <q>Why should we build our own LLMs?</q> should be <q>Why should you build your own LLM?</q></p>

	<p><strong>Chapter 1, Section <q>1.3 Stages of building and using LLMs</q>, page 6</p></strong>
	<p>In NOTE: <q>However, this is not...</q> should be <q>This is not..</q></p>

	<p><strong>Chapter 1, Section <q>1.3 Stages of building and using LLMs</q>, page 7</p></strong>
	<p>After NOTE, Paragraph 2: <q>After obtaining a pretrained LLM from training...</q> should be <q>After obtaining a pretrained LLM by training...</q></p>

	<p><strong>Chapter 1, Section <q>1.6 A closer look at the GPT architecture</q>, page 13</p></strong>
	<p>Paragraph 4: <q>Hence, understanding GPT remains as relevant as ever, so I focus on implementing the prominent architecture behind GPT while providing pointers to specific tweaks employed by alternative LLMs.</q> should be <q>Understanding GPT remains highly relevant. I focus on implementing the prominent architecture behind GPT and provide pointers to specific tweaks used by alternative LLMs.</q></p>

	<p><strong>Chapter 2, Section <q>2.2 Tokenizing text</q>, page 22</p></strong>
	<p>After Listing 2.1, Paragraph 1: <q>...followed by the first 100...</q> should be <q>...followed by the first 99...</q></p>

	<p><strong>Chapter 2, Section <q>2.3 Converting tokens into token IDs</q>, page 25</p></strong>
	<p>In Figure 2.6 description: <q>The depicted vocabulary is purposefully small...</q> should be <q>The depicted vocabulary is purposely small...</q></p>

	<p><strong>Chapter 2, Section <q>2.7 Creating token embeddings</q>, page 42</p></strong>
	<p>In NOTE: <q>...section B.4...</q> should be <q>...section A.4...</q></p>

	<p><strong>Chapter 3, Section <q>3.2 Capturing data dependencies with attention mechanisms</q>, page 54</p></strong>
	<p>In Figure 3.5: <q>...a way to access to all input tokens.</q> should be <q>...a way to access all input tokens.</q></p>

	<p><strong>Chapter 3, Section <q>3.3.1 A simple self-attention mechanism without trainable weights</q>, page 61</p></strong>
	<p>In Figure 3.10: <q>a<sub>24</sub></q> should be <q>a<sub>2T</sub></q></p>

	<p><strong>Chapter 3, Section <q>3.4.2 Implementing a compact self-attention Python class</q>, page 72</p></strong>
	<p>In Figure 3.18 description: <q>The new compute the attention...</q> should be <q>Then we compute the attention...</q></p>

	<p><strong>Chapter 4, Section <q>4.7 Generating text</q>, page 124</p></strong>
	<p>In Figure 4.17: <q>5. Indentifies</q> should be <q>5. Identifies</q></p>

	<p><strong>Chapter 5, Section <q>5.2 Training an LLM</q>, page 151</p></strong>
	<p>After Figure 5.13, Paragraph 1: <q>...our objectives for this chaper.</q> should be <q>...our objectives for this chapter.</q></p>

	<p><strong>Chapter 6, Section <q>6.3 Creating data loaders</q>, page 176</p></strong>
	<p>After Figure 6.6, Paragraph 1: <q>...it identifies the longest sequence in the training dataset, encodes the text messages, and...</q> should be <q>...it encodes the text messages into token sequences, identifies the longest sequence in the training dataset, and...</q></p>

	<p><strong>Chapter 6, Section <q>6.8 Using the LLM as a spam classifier</q>, page 201</p></strong>
	<p>Within Listing 6.12: <code>model.pos_emb.weight.shape[1]</code> should be <code>model.pos_emb.weight.shape[0]</code></p>

	<p><strong>Chapter 7, Section <q>7.2 Preparing a dataset for supervised instruction fine-tuning</q>, page 207</p></strong>
	<p>Within Listing 7.1, delete code lines: <code>else:<br><in>with open(file_path, "r", encoding="utf-8") as file:<br><in><in>text_data = file.read()</code><br>and Annotation 1.</p>

	<p><strong>Chapter 7, Section <q>7.3 Organizing data into training batches</q>, page 213</p></strong>
	<p>In Figure 7.7: <code>"Ocassion",</code> should be <code>"",</code></p>

	<p><strong>Chapter 7, Section <q>7.7 Extracting and saving responses</q>, page 236</p></strong>
	<p>In Paragraph 2 from the top of the page: <q>...as it is for completion...</q> should be <q>...as it is for classification...</q></p>

	<p><strong>Appendix A, Section <q>A.2 Understanding tensors</q>, page 258</p></strong>
	<p>After Figure A.6, Paragraph 2: <q>A.8</q> should be <q>A.9</q></p>

	<p><strong>Appendix A, Section <q>A.2.3 Common PyTorch tensor operations</q>, page 261</p></strong>
	<p>Paragraph 2 from the top of the page: <q>Note that this is similar to...</q> should be <q>Note that this is not the same as...</q></p>

	<p><strong>Appendix A, Section <q>A.4 Automatic differentiation made easy</q>, page 264</p></strong>
	<p>After Listing A.3, Paragraph 1: <q>The resulting values of the loss...</q> should be <q>The resulting values of the loss gradients...</q></p>

	<p><strong>Appendix A, Section <q>A.8 Saving and loading models</q>, page 278</p></strong>
	<p>Paragraph 1: <q>Here’s the recommended way how we can save and load...</q> should be <q>Here’s the recommended way of saving and loading...</q></p>

	<p><strong>Appendix A, Section <q>A.9.3 Training with multiple GPUs</q>, page 285</p></strong>
	<p>In Listing A.13, Annotation 7: <q>Distibuted-Sampler takes care of the shuffling now.</q> should be <q>Distributed-Sampler takes care of the shuffling now.</q></p>

	<p><strong>Appendix B, Section <q>Chapter 4</q>, page 293</p></strong>
	<p><q>https://mng.bz/lMgo</q> should be <q>https://mng.bz/DMv0</q></p>

	<p><strong>Appendix C, Section <q>Exercise A.1</q>, page 310</p></strong>
	<p>The Exercise answer should be replaced with: <q>The optional Python Setup Tips document (https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/01_optional-python-setup-preferences) contains additional recommendations and tips if you need additional help to set up your Python environment.</q></p>

	<p><strong>Appendix C, Section <q>Exercise A.2</q>, page 311</p></strong>
	<p>The Exercise answer should be replaced with: <q>The the optional <q>Installing Libraries Used In This Book</q> document (https://github.com/rasbt/LLMs-from-scratch/tree/main/setup/02_installing-python-libraries) contains utilities to check whether your environment is set up correctly.</q></p>

	<p><strong>Appendix D, Section <q>D.1 Learning rate warmup</q>, page 315</p></strong>
	<p>First code snippet, delete the last code line <code>warmup_steps = 20</code></p>

	<p><strong>Appendix D, Section <q>D.4 The modified training function</q>, page 320</p></strong>
	<p>In first code snippet, code line: <code>if global_step > warmup_steps:</code> should be <code>if global_step >= warmup_steps:</code></p>

	<p><strong>Appendix D, Section <q>D.4 The modified training function</q>, page 320</p></strong>
	<p>In second code snippet, code line: <code>peak_lr = 5e-4</code> should be <code>peak_lr = 0.001</code></p>

	<p><strong>Appendix D, Section <q>D.4 The modified training function</q>, page 321</p></strong>
	<p>In third code snippet, code line: <code>Train loss 0.041</code> should be <code>Train loss 0.035</code> AND <code>Val loss 6.915</code> should be <code>Val loss 6.938</code></p>

	<p><strong>Appendix E, Section <q>E.4 Parameter-efficient fine-tuning with LoRA</q>, page 330</p></strong>
	<p>Listing E.6 description is incorrect: <q>Listing E.6 Replacing a LinearWithLora layer with Linear layers</q> should be <q>Listing E.6 Replacing Linear layers with LinearWithLora layers</q></p>
                        
  </div>
  <div class="copyright">
    &copy; 2025 Manning Publications Co. All rights reserved.
  </div>
</body>
</html>
